# System
use_gpu: true                   # Whether to train the model on GPU.
multi_gpu: false                # Ensure multi-GPU training.
device_ids: [0]                 # GPU index.
seed: 14                        # Random seed for initialization.

# Dataset
dataset: "cct"                # Dataset name. Options: ['cct']

# Pre-trained Model
task_type: "token_cls"          # NLP task type. Options: ['token_cls']
ptm_name: "code5t-base"         # Pre-trained model name. Options: ['code5t-base', 'codebert-base', 'graphcodebert-base', 'unixcoder-base']
ptm_type: "RoBERTa"             # Pre-trained model type. Options: ['Auto', 'BERT', 'RoBERTa']

# Model
rnn: "LSTM"                     # RNN type. Options: ['RNN', 'GRU', 'LSTM']
crf: true                       # Whether to use CRF model.

model_name: "tner"               # Name of model. Options: ['tner', 'tir']
model_args:                     # Args of Model (Customize based on your model's requirements).
  hid_dim: 768
  dropout: 0.2

# Train
do_train: true                   # Whether to train a model from scratch.
epochs: 120                      # Number of training epochs.
train_batch_size: 128             # Batch size of training.
eval_batch_size: 128              # Batch size of evaluating.
eval_per_epoch: 1                # How often evaluating the trained model on valid dataset during training.
optimizer: "SGD"                 # Optimizer type. Options: ['SGD', 'Adam']
learning_rate: 0.00005             # Learning rate.
lr_factor: 0.00005                 # Learning rate factor.
crf_learning_rate: 0.00005         # CRF learning rate.
dropout_rate: 0.2                # Dropout rate.
warmup_proportion: 0.1           # Warmup proportion.
weight_decay: 0.01               # Weight decay.
gradient_accumulation_steps: 1    # Gradient accumulation steps.
max_grad_norm: 1.0               # Max gradient norm.
max_seq_len: 512                 # Max sequence length.


